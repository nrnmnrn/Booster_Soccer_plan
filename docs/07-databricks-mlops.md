# Databricks MLOps

## æ¦‚è¿°

æœ¬æ–‡ä»¶èªªæ˜å¦‚ä½•ä½¿ç”¨ Databricks Jobs/Workflows å»ºç«‹ç«¯åˆ°ç«¯çš„ RL è¨“ç·´ Pipelineï¼ŒåŒ…å« Unity Catalog æ•´åˆã€GPU ç›£æ§å’Œè‡ªå‹•åŒ–æµç¨‹ã€‚

**ç›®æ¨™ï¼š** å»ºç«‹å¯é‡ç¾ã€å¯è¿½è¹¤ã€å¯æ“´å±•çš„ RL è¨“ç·´åŸºç¤è¨­æ–½ã€‚

---

## ç®—åŠ›ç­–ç•¥ï¼šé–‹ç™¼æ±‚ç©©ï¼Œè¨“ç·´æ±‚çœ

ç‚ºäº†åœ¨æœ‰é™é ç®—ä¸‹æ¥µå¤§åŒ–å¯¦é©—æ¬¡æ•¸ï¼Œæˆ‘å€‘å°‡é–‹ç™¼èˆ‡è¨“ç·´éšæ®µçš„ç®—åŠ›é‚è¼¯å¾¹åº•åˆ†é›¢ã€‚

### é–‹ç™¼éšæ®µï¼šAll-purpose Cluster (On-demand)

| é …ç›® | é…ç½® |
|------|------|
| **ç”¨é€”** | Gate 1/2 é©—è­‰ã€Debugã€Notebook å¯¦é©— |
| **é¡å‹** | All-purpose Compute (Single Node) |
| **ç¡¬é«”** | NVIDIA L4 GPU (On-demand) |
| **æ©Ÿåˆ¶** | Auto-termination = 60 åˆ†é˜ |

**é¸æ“‡åŸå› ï¼š**
- **å¿«å•Ÿå‹•å„ªåŒ–ï¼š** Databricks åœ¨ All-purpose æ¨¡å¼ä¸‹å…·å‚™é€²ç¨‹ç´šé‡å•Ÿèƒ½åŠ›ã€‚åœ¨å¯†é›† Debug ä»£ç¢¼æ™‚ï¼Œé‡å•Ÿåªéœ€ 10-20 ç§’ï¼Œç„¡éœ€åƒ Pool ä¸€æ¨£æ”¯ä»˜é–’ç½® VM ç§Ÿé‡‘å³å¯ç²å¾—æ¥µä½³çš„åæ‡‰é€Ÿåº¦ã€‚
- **ç’°å¢ƒä¸€è‡´æ€§ï¼š** æ­é… Docker Container Service (DCS)ï¼Œç’°å¢ƒå·²å›ºåŒ–ï¼Œæ¸›å°‘å†·å•Ÿå‹•æ™‚çš„å®‰è£ç­‰å¾…ã€‚

### è¨“ç·´éšæ®µï¼šJob Cluster + Instance Pool (Spot)

| é …ç›® | é…ç½® |
|------|------|
| **ç”¨é€”** | Job 2 (é è¨“ç·´)ã€Job 4 (å¾®èª¿) |
| **é¡å‹** | Train-Pool + Spot Instances |
| **ç¡¬é«”** | NVIDIA L4 GPU (Preemptible) |

**æˆæœ¬å„ªå‹¢ï¼š**
| é …ç›® | æ•ˆç›Š |
|------|------|
| DBU è²»ç‡ | Jobs Workload åƒ…ç‚º All-purpose çš„ ~1/3 |
| ç¡¬é«”å–®åƒ¹ | Spot æ¯” On-demand ä¾¿å®œ 70-80% |
| ç¶œåˆé ç®— | åŒæ¨£çš„éŒ¢å¯å¤šè·‘ **4-5 å€**å¯¦é©— |

**é¸æ“‡åŸå› ï¼š**
- **è‡ªå‹•æ¢å¾©èƒ½åŠ›ï¼š** é…åˆ Unity Catalog Volumes å­˜å„² Checkpointsã€‚å³ä¾¿ Spot æ©Ÿå™¨è¢«å›æ”¶ï¼ŒJob æœƒè‡ªå‹•é‡è©¦ï¼Œè…³æœ¬æœƒåµæ¸¬æœ€æ–°æ¬Šé‡ä¸¦å¯¦ç¾ã€Œç„¡æ„ŸçºŒç·´ã€ã€‚
- **Warm Start å„ªå‹¢ï¼š** è¨­ç½® `idle_instance_autotermination_minutes = 60`ã€‚ç•¶ä¸€å€‹ 4 å°æ™‚çš„è¨“ç·´ Job å®Œæˆå¾Œï¼Œæ©Ÿå™¨æœƒç•™åœ¨æ± å­è£¡ä¸€å°æ™‚ã€‚å¦‚æœä½ ç«‹å³èª¿æ•´åƒæ•¸å•Ÿå‹•ä¸‹ä¸€å ´å¯¦é©—ï¼Œå°‡äº«å—ç§’ç´šé–‹æ©Ÿã€‚

---

## Train-Pool é…ç½®

### Instance Pool å®šç¾©

```json
{
  "instance_pool_name": "booster-train-pool",
  "node_type_id": "g2-standard-12",
  "min_idle_instances": 0,
  "max_capacity": 2,
  "idle_instance_autotermination_minutes": 60,
  "preloaded_spark_versions": ["16.4-gpu-ml-scala2.12"],
  "gcp_attributes": {
    "availability": "PREEMPTIBLE_GCP"
  }
}
```

### è¨­è¨ˆç†ç”±

| åƒæ•¸ | å€¼ | åŸå›  |
|------|-----|------|
| `min_idle_instances` | 0 | ä¸ç”¢ç”Ÿé–’ç½®è²»ç”¨ |
| `idle_instance_autotermination_minutes` | 60 | Warm Startï¼šé€£çºŒå¯¦é©—ç§’ç´šé–‹æ©Ÿ |
| `max_capacity` | 2 | å…è¨±åŒæ™‚è·‘ 2 å€‹å¯¦é©— |
| `preloaded_spark_versions` | 16.4-gpu-ml | é è¼‰ Runtime åŠ é€Ÿå•Ÿå‹• |

### å»ºç«‹ Instance Pool

```bash
# ä½¿ç”¨ Databricks CLI
databricks instance-pools create --json '{
  "instance_pool_name": "booster-train-pool",
  "node_type_id": "g2-standard-12",
  "min_idle_instances": 0,
  "max_capacity": 2,
  "idle_instance_autotermination_minutes": 60,
  "preloaded_spark_versions": ["16.4-gpu-ml-scala2.12"],
  "gcp_attributes": {
    "availability": "PREEMPTIBLE_GCP"
  }
}'
```

---

## Jobs/Workflows Pipeline æ¶æ§‹

### æ•´é«”æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATABRICKS WORKFLOWS PIPELINE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  [Job 1: Setup]  â†’  [Job 2: MJX Pre-train]  â†’  [Job 3: Conversion]  â”‚
â”‚       (CPU)             (L4 GPU)                    (CPU)             â”‚
â”‚     ~30 min              ~4 hrs                    ~10 min            â”‚
â”‚                              â”‚                                        â”‚
â”‚                              â†“                                        â”‚
â”‚                    [Job 4: Fine-tune]  â†’  [Job 5: Submit]            â”‚
â”‚                         (L4 GPU)              (CPU)                   â”‚
â”‚                          ~3 hrs               ~5 min                  â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Job å®šç¾©

| Job | ç›®çš„ | Cluster é¡å‹ | é ä¼°æ™‚é–“ | è¼¸å‡º |
|-----|------|-------------|---------|------|
| **Job 1: Setup** | ç’°å¢ƒé©—è­‰ã€Unity Catalog è¨­ç½® | CPU | 30 min | Volumes å»ºç«‹ |
| **Job 2: MJX Pre-train** | JAX SAC é è¨“ç·´ (10M æ­¥) | L4 GPU | 2-4 hrs | `final_checkpoint.pkl` |
| **Job 3: Conversion** | JAX â†’ PyTorch è½‰æ› | CPU | 10 min | `model_pretrained.pt` |
| **Job 4: Fine-tune** | DDPG å®˜æ–¹ç’°å¢ƒå¾®èª¿ | L4 GPU | 2-3 hrs | `model_finetuned.pt` |
| **Job 5: Submit** | Benchmark + SAI æäº¤ | CPU | 5 min | ç«¶è³½åˆ†æ•¸ |

---

## Job è©³ç´°è¦åŠƒ

### Job 1: Environment Setup

**ç›®çš„ï¼š** é©—è­‰ç’°å¢ƒã€å»ºç«‹ Unity Catalog çµæ§‹ã€ç¢ºèª GPU å¯ç”¨

**æ­¥é©Ÿï¼š**
1. å®‰è£ä¾è³´ (`mujoco`, `mujoco-mjx`, `jax[cuda12]`, `wandb`, `mlflow`)
2. é©—è­‰ JAX å¯è¦‹ GPU
3. å»ºç«‹ Unity Catalog Schema å’Œ Volumes
4. é©—è­‰ XML å ´æ™¯å¯è¼‰å…¥
5. æ¸¬è©¦ W&B é€£ç·š
6. è¨˜éŒ„è¨­ç½®çµæœåˆ° MLflow

**Cluster é…ç½®ï¼š**
```json
{
  "spark_version": "16.4-cpu-ml-scala2.12",
  "node_type_id": "n2-standard-4",
  "num_workers": 0,
  "timeout_seconds": 1800
}
```

**ç”¢å‡ºï¼š**
- Unity Catalog Volumes å»ºç«‹å®Œæˆ
- ç’°å¢ƒé©—è­‰å ±å‘Š (MLflow logged)

---

### Job 2: MJX Pre-training

**ç›®çš„ï¼š** ä½¿ç”¨ MJX GPU åŠ é€Ÿé€²è¡Œ SAC é è¨“ç·´

**æ­¥é©Ÿï¼š**
1. åˆå§‹åŒ– W&B + MLflow é›™é‡è¨˜éŒ„
2. è¼‰å…¥ MJX ç’°å¢ƒ (2048 ä¸¦è¡Œ)
3. åŸ·è¡Œ SAC è¨“ç·´å¾ªç’° (10M æ­¥)
4. æ¯ 1000 æ­¥è¨˜éŒ„ metrics
5. æ¯ 5000 æ­¥è¨˜éŒ„ GPU ç‹€æ…‹
6. æ¯ 200k æ­¥ä¿å­˜ checkpointï¼ˆé…åˆ Preemptible ç¸®çŸ­é–“éš”ï¼‰
7. æœ€çµ‚æ¨¡å‹è¨»å†Šåˆ° Unity Catalog

**Cluster é…ç½®ï¼ˆä½¿ç”¨ Train-Pool + Dockerï¼‰ï¼š**
```json
{
  "instance_pool_id": "<TRAIN_POOL_ID>",
  "num_workers": 0,
  "spark_version": "16.4-gpu-ml-scala2.12",
  "docker_image": {
    "url": "your-registry/booster-rl:v1"
  },
  "timeout_seconds": 28800
}
```

> **æ³¨æ„**ï¼šä½¿ç”¨ Instance Pool æ™‚ï¼Œ`node_type_id` å’Œ `gcp_attributes` ç”± Pool å®šç¾©ï¼Œç„¡éœ€åœ¨ Job é…ç½®ä¸­æŒ‡å®šã€‚

**Job é‡è©¦é…ç½®ï¼ˆé…åˆ Preemptibleï¼‰ï¼š**
```json
{
  "max_retries": 2,
  "retry_on_timeout": true
}
```

**å‚™ç”¨æ–¹æ¡ˆï¼šInit Script (install_mjx.sh)**

> âš ï¸ **å»ºè­°å„ªå…ˆä½¿ç”¨ Docker**ï¼šè©³è¦‹ [01-environment-setup.md](./01-environment-setup.md)ã€‚

```bash
#!/bin/bash
# æ ¸å¿ƒå¥—ä»¶å®‰è£ï¼ˆç‰ˆæœ¬èˆ‡ Dockerfile ä¿æŒä¸€è‡´ï¼‰
pip install --no-cache-dir \
  "mujoco==3.2.6" \
  "mujoco-mjx==3.2.6" \
  "brax==0.12.1" \
  "optax==0.2.4" \
  "wandb==0.19.1" \
  "mlflow==2.19.0" \
  "pynvml>=12.0.0"

# PyTorchï¼ˆç”¨æ–¼æœ€çµ‚å¾®èª¿ï¼‰
pip install --no-cache-dir torch==2.6.0 --index-url https://download.pytorch.org/whl/cu124

# JAX/XLA è¨˜æ†¶é«”å’Œæ•ˆèƒ½è¨­ç½®
echo "export XLA_PYTHON_CLIENT_MEM_FRACTION=0.75" >> /etc/profile
echo "export JAX_PREALLOCATE=false" >> /etc/profile
echo "export XLA_FLAGS='--xla_gpu_cuda_data_dir=/usr/local/cuda'" >> /etc/profile
echo "export MUJOCO_GL=egl" >> /etc/profile
```

**ç›£æ§é …ç›®ï¼š**
- `train/reward_mean`
- `train/critic_loss`
- `train/actor_loss`
- `gpu/memory_percent`
- `gpu/utilization_percent`

**ç”¢å‡ºï¼š**
- `/Volumes/booster_soccer/rl_models/checkpoints/mjx_pretraining/final_checkpoint.pkl`
- `booster_soccer.rl_models.mjx_sac_pretrained` (Model Registry)

---

### Job 3: Model Conversion

**ç›®çš„ï¼š** å°‡ JAX SAC Actor è½‰æ›ç‚º PyTorch DDPG æ ¼å¼

**æ­¥é©Ÿï¼š**
1. è¼‰å…¥ JAX checkpoint
2. æå– Actor æ¬Šé‡ (åªå– mean éƒ¨åˆ†ï¼Œæ¨æ£„ log_std)
3. è½‰æ›ç‚º PyTorch state_dict
4. é©—è­‰ç¶­åº¦æ­£ç¢º (æœ€å¾Œä¸€å±¤ 12 ç¶­ï¼Œä¸æ˜¯ 24 ç¶­)
5. ä¿å­˜è½‰æ›å¾Œæ¨¡å‹
6. è¨˜éŒ„è½‰æ›çµæœåˆ° MLflow

**é—œéµè½‰æ›é‚è¼¯ï¼š**
```
SAC Actor è¼¸å‡º: 24 (mean:12 + log_std:12)
                    â†“
                åªå–å‰ 12 ç¶­
                    â†“
DDPG Actor è¼¸å‡º: 12
```

**é©—è­‰æª¢æŸ¥ï¼š**
- `layers.2.weight.shape == (12, 256)` â† Critical!
- `layers.2.bias.shape == (12,)`

**ç”¢å‡ºï¼š**
- `/Volumes/booster_soccer/rl_models/checkpoints/pytorch_finetuning/model_pretrained.pt`
- `booster_soccer.rl_models.ddpg_pretrained` (Model Registry)

---

### Job 4: PyTorch Fine-tuning

**ç›®çš„ï¼š** åœ¨å®˜æ–¹ç’°å¢ƒä¸­ä½¿ç”¨ DDPG å¾®èª¿é è¨“ç·´æ¨¡å‹

**æ­¥é©Ÿï¼š**
1. è¼‰å…¥é è¨“ç·´æ¬Šé‡
2. è¨­ç½® Feature Freeze Scheduler (ä¸‰éšæ®µ)
3. è¨­ç½® Reward Annealer (Dense â†’ Sparse)
4. åŸ·è¡Œ DDPG è¨“ç·´å¾ªç’° (200k æ­¥)
5. è¨˜éŒ„ metrics åˆ° W&B + MLflow
6. ä¿å­˜æœ€çµ‚æ¨¡å‹

**Feature Freeze ä¸‰éšæ®µï¼š**

| éšæ®µ | Steps | å¯è¨“ç·´å±¤ | Learning Rate |
|------|-------|---------|---------------|
| Phase 1 | 0 - 20k | æœ€å¾Œä¸€å±¤ | 3e-5 |
| Phase 2 | 20k - 50k | æœ€å¾Œå…©å±¤ | 3e-5 |
| Phase 3 | 50k+ | å…¨ç¶²è·¯ | 1e-5 (é™ä½) |

**Reward Annealingï¼š**
```
R_total = Î± Ã— R_dense + Î² Ã— R_official

é–‹å§‹: Î± = 1.0, Î² = 0.1
çµæŸ: Î± = 0.1, Î² = 1.0
```

**ç”¢å‡ºï¼š**
- `/Volumes/booster_soccer/rl_models/checkpoints/pytorch_finetuning/model_finetuned.pt`
- `booster_soccer.rl_models.ddpg_finetuned` (Model Registry)

---

### Job 5: SAI Submission

**ç›®çš„ï¼š** æœ¬åœ° benchmark ä¸¦æäº¤åˆ° SAI ç«¶è³½

**æ­¥é©Ÿï¼š**
1. è¼‰å…¥å¾®èª¿å¾Œæ¨¡å‹
2. åœ¨ä¸‰å€‹ç’°å¢ƒåŸ·è¡Œ benchmark
3. è¨˜éŒ„ benchmark çµæœåˆ° MLflow
4. æäº¤åˆ° SAI å¹³å°
5. è¨˜éŒ„æäº¤ ID

**Benchmark ç’°å¢ƒï¼š**
- `LowerT1GoaliePenaltyKick-v0`
- `LowerT1ObstaclePenaltyKick-v0`
- `LowerT1KickToTarget-v0`

**ç”¢å‡ºï¼š**
- Benchmark çµæœ (MLflow metrics)
- SAI æäº¤è¨˜éŒ„

---

## Workflow ç·¨æ’

### Multi-Task Job å®šç¾©

```json
{
  "name": "booster_soccer_pipeline",
  "tasks": [
    {
      "task_key": "setup",
      "notebook_task": {
        "notebook_path": "/databricks/workflows/01_environment_setup"
      }
    },
    {
      "task_key": "pretrain",
      "depends_on": [{"task_key": "setup"}],
      "notebook_task": {
        "notebook_path": "/databricks/workflows/02_mjx_pretraining"
      }
    },
    {
      "task_key": "convert",
      "depends_on": [{"task_key": "pretrain"}],
      "notebook_task": {
        "notebook_path": "/databricks/workflows/03_model_conversion"
      }
    },
    {
      "task_key": "finetune",
      "depends_on": [{"task_key": "convert"}],
      "notebook_task": {
        "notebook_path": "/databricks/workflows/04_pytorch_finetuning"
      }
    },
    {
      "task_key": "submit",
      "depends_on": [{"task_key": "finetune"}],
      "notebook_task": {
        "notebook_path": "/databricks/workflows/05_sai_submission"
      }
    }
  ],
  "email_notifications": {
    "on_failure": ["your-email@example.com"]
  }
}
```

### æ‰‹å‹•è§¸ç™¼ vs è‡ªå‹•æ’ç¨‹

| æ¨¡å¼ | ä½¿ç”¨å ´æ™¯ | é…ç½® |
|------|----------|------|
| **æ‰‹å‹•è§¸ç™¼** | é–‹ç™¼è¿­ä»£ã€èª¿è©¦ | `databricks jobs run-now --job-id <ID>` |
| **è‡ªå‹•æ’ç¨‹** | æ¯æ—¥è¨“ç·´ã€å®šæœŸæ›´æ–° | `schedule: { "quartz_cron_expression": "0 0 2 * * ?" }` |

---

## æˆæœ¬å„ªåŒ–ç­–ç•¥

### GCP Preemptible Instancesï¼ˆæ¨è–¦ï¼‰

ä½¿ç”¨ Preemptible VM å¯ç¯€çœç´„ **56-70%** çš„é‹ç®—æˆæœ¬ï¼š

| é¡å‹ | L4 GPU åƒ¹æ ¼ (æ¯å°æ™‚) | 10 å°æ™‚è¨“ç·´æˆæœ¬ |
|------|---------------------|----------------|
| **On-Demand** | ~$0.80 | ~$8.00 |
| **Preemptible** | ~$0.35 | ~$3.50 |

**é…ç½®æ–¹å¼ï¼ˆå–®ç¯€é» GPU Clusterï¼‰ï¼š**
```json
{
  "gcp_attributes": {
    "availability": "PREEMPTIBLE_GCP"
  }
}
```

> âš ï¸ **æ³¨æ„**ï¼šå°æ–¼å–®ç¯€é»ï¼ˆç„¡ Workerï¼‰Clusterï¼Œå¿…é ˆä½¿ç”¨ `availability` è€Œé `use_preemptible_executors`ã€‚å¾Œè€…åªå½±éŸ¿ Worker ç¯€é»ã€‚

**é¢¨éšªèˆ‡ç·©è§£ï¼š**
| é¢¨éšª | ç·©è§£æªæ–½ |
|------|----------|
| è¢«æ¶ä½”ä¸­æ–·è¨“ç·´ | ç¸®çŸ­ Checkpoint é–“éš”è‡³ 200k æ­¥ |
| é€²åº¦ä¸Ÿå¤± | è¨­ç½® `max_retries: 2` è‡ªå‹•é‡è©¦ |
| é »ç¹è¢«æ¶ä½” | å˜—è©¦ä¸åŒæ™‚æ®µå•Ÿå‹•ï¼ˆéå°–å³°æ™‚æ®µï¼‰ |

---

### Cluster è‡ªå‹•çµ‚æ­¢

```python
# åœ¨ Notebook çµå°¾
dbutils.notebook.exit(json.dumps({
    "status": "success",
    "terminate_cluster": True
}))
```

### GPU Job å„ªåŒ–

| ç­–ç•¥ | æ•ˆæœ | å¯¦ä½œ |
|------|------|------|
| **Checkpoint Recovery** | é¿å…é‡è¤‡è¨“ç·´ | è‡ªå‹•åµæ¸¬æœ€æ–° checkpointï¼ˆæ¯ 200k æ­¥ï¼‰ |
| **Early Stopping** | ç¯€çœç„¡æ•ˆè¨“ç·´æ™‚é–“ | W&B alert + æ‰‹å‹•åœæ­¢ |
| **Batch Size æœ€å¤§åŒ–** | æé«˜ GPU åˆ©ç”¨ç‡ | 2048 ä¸¦è¡Œç’°å¢ƒ |
| **Preemptible + Retry** | ç¯€çœ 56-70% æˆæœ¬ | `availability: PREEMPTIBLE_GCP` |

### Checkpoint Recovery é‚è¼¯

#### åŸå­æ€§å¯«å…¥ï¼ˆé˜²æ­¢ Spot æ¶ä½”æå£ï¼‰

```python
import os
import pickle
import tempfile
import shutil
import re

def save_checkpoint_atomic(params, checkpoint_dir, step, keep_last=2):
    """
    åŸå­æ€§å¯«å…¥ checkpointï¼Œé˜²æ­¢ Spot æ¶ä½”å°è‡´æª”æ¡ˆæå£

    ç­–ç•¥ï¼š
    1. å…ˆå¯«å…¥ temp æª”æ¡ˆ
    2. æˆåŠŸå¾Œ rename ç‚ºæ­£å¼æª”æ¡ˆï¼ˆåŸå­æ“ä½œï¼‰
    3. æ¸…ç†èˆŠ checkpointï¼Œåªä¿ç•™æœ€è¿‘ N å€‹

    Args:
        params: è¦ä¿å­˜çš„æ¨¡å‹åƒæ•¸
        checkpoint_dir: checkpoint ç›®éŒ„
        step: ç•¶å‰è¨“ç·´æ­¥æ•¸
        keep_last: ä¿ç•™æœ€è¿‘å¹¾å€‹ checkpointï¼ˆé è¨­ 2ï¼‰

    Returns:
        ä¿å­˜çš„ checkpoint è·¯å¾‘
    """
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint_path = f"{checkpoint_dir}/step_{step}.pkl"

    # 1. å¯«å…¥ temp æª”æ¡ˆï¼ˆåŒä¸€ç›®éŒ„ç¢ºä¿åœ¨åŒä¸€æª”æ¡ˆç³»çµ±ï¼‰
    with tempfile.NamedTemporaryFile(
        mode='wb',
        dir=checkpoint_dir,
        delete=False,
        suffix='.tmp'
    ) as tmp:
        pickle.dump(params, tmp)
        tmp_path = tmp.name

    # 2. åŸå­ renameï¼ˆPOSIX ç³»çµ±ä¸Š rename æ˜¯åŸå­æ“ä½œï¼‰
    shutil.move(tmp_path, checkpoint_path)

    # 3. æ¸…ç†èˆŠ checkpoint
    _cleanup_old_checkpoints(checkpoint_dir, keep_last)

    return checkpoint_path


def _cleanup_old_checkpoints(checkpoint_dir, keep_last):
    """ä¿ç•™æœ€è¿‘ N å€‹ checkpointï¼Œåˆªé™¤å…¶ä»–"""
    checkpoints = sorted([
        f for f in os.listdir(checkpoint_dir)
        if f.startswith("step_") and f.endswith(".pkl")
    ], key=lambda x: int(re.search(r'step_(\d+)', x).group(1)), reverse=True)

    for ckpt in checkpoints[keep_last:]:
        try:
            os.remove(os.path.join(checkpoint_dir, ckpt))
        except OSError:
            pass  # å¿½ç•¥åˆªé™¤å¤±æ•—
```

#### è¼‰å…¥ Checkpointï¼ˆæ”¯æ´ Fallbackï¼‰

```python
def load_latest_checkpoint(checkpoint_dir, fallback_count=2):
    """
    è¼‰å…¥æœ€æ–° checkpointï¼Œæ”¯æ´ fallback åˆ°è¼ƒèˆŠç‰ˆæœ¬

    ç•¶æœ€æ–° checkpoint æå£æ™‚ï¼ˆSpot æ¶ä½”å°è‡´ï¼‰ï¼Œè‡ªå‹•å˜—è©¦è¼‰å…¥è¼ƒèˆŠçš„ç‰ˆæœ¬ã€‚

    Args:
        checkpoint_dir: checkpoint ç›®éŒ„
        fallback_count: æœ€å¤šå˜—è©¦çš„ checkpoint æ•¸é‡

    Returns:
        (params, step) æˆ– (None, 0)
    """
    if not os.path.exists(checkpoint_dir):
        return None, 0

    checkpoints = sorted([
        f for f in os.listdir(checkpoint_dir)
        if f.startswith("step_") and f.endswith(".pkl")
    ], key=lambda x: int(re.search(r'step_(\d+)', x).group(1)), reverse=True)

    if not checkpoints:
        return None, 0

    for i, ckpt in enumerate(checkpoints[:fallback_count]):
        ckpt_path = os.path.join(checkpoint_dir, ckpt)
        try:
            with open(ckpt_path, 'rb') as f:
                params = pickle.load(f)
            step = int(re.search(r'step_(\d+)', ckpt).group(1))
            if i > 0:
                print(f"âš ï¸ æœ€æ–° checkpoint æå£ï¼Œå·²è¼‰å…¥ {ckpt}")
            return params, step
        except (EOFError, pickle.UnpicklingError, OSError) as e:
            print(f"âš ï¸ Checkpoint {ckpt} æå£: {e}ï¼Œå˜—è©¦ä¸Šä¸€å€‹...")
            continue

    print("âŒ æ‰€æœ‰ checkpoint éƒ½ç„¡æ³•è¼‰å…¥ï¼Œå¾é ­é–‹å§‹è¨“ç·´")
    return None, 0
```

#### ä½¿ç”¨ç¯„ä¾‹

```python
# åœ¨è¨“ç·´è…³æœ¬ä¸­
checkpoint_dir = "/Volumes/booster_soccer/rl_models/checkpoints/mjx_pretraining"

# å˜—è©¦è¼‰å…¥ä¹‹å‰çš„é€²åº¦
params, start_step = load_latest_checkpoint(checkpoint_dir)
if params is not None:
    print(f"âœ… å¾ step {start_step} ç¹¼çºŒè¨“ç·´")
else:
    print("ğŸ†• å¾é ­é–‹å§‹è¨“ç·´")
    params = init_params()
    start_step = 0

# è¨“ç·´å¾ªç’°
for step in range(start_step, total_steps):
    # ... è¨“ç·´é‚è¼¯ ...

    # æ¯ 200k æ­¥ä¿å­˜ checkpointï¼ˆåŸå­å¯«å…¥ï¼‰
    if step % 200_000 == 0 and step > 0:
        save_checkpoint_atomic(params, checkpoint_dir, step, keep_last=2)
```

---

## éŒ¯èª¤è™•ç†èˆ‡è­¦å ±

### å¤±æ•—é€šçŸ¥

```json
{
  "email_notifications": {
    "on_failure": ["team@example.com"],
    "on_success": ["team@example.com"]
  }
}
```

### W&B è­¦å ±æ•´åˆ

```python
# GPU è¨˜æ†¶é«”è­¦å ±
if gpu_memory_percent > 95:
    wandb.alert(
        title="GPU Memory Critical",
        text=f"Memory at {gpu_memory_percent}%",
        level=wandb.AlertLevel.ERROR
    )

# è¨“ç·´åœæ»¯è­¦å ±
if reward_moving_avg < threshold:
    wandb.alert(
        title="Training Stalled",
        text="Reward not improving for 100k steps"
    )
```

---

## æ¨¡å‹ Lineage è¿½è¹¤

### é›™è»Œè¿½è¹¤ç­–ç•¥

**è¨­è¨ˆåŸå‰‡ï¼š**
- **Run (æ­·å²è¿½è¹¤)**ï¼šæ¯å€‹ Job éƒ½è¨˜éŒ„ï¼Œç¢ºä¿å®Œæ•´ Lineage
- **Registry (é‡Œç¨‹ç¢‘)**ï¼šåªåœ¨é—œéµç¯€é»è¨»å†Šï¼Œä¿æŒ Catalog æ•´æ½”

> ã€Œæ­·å²è¦ç´€éŒ„åœ¨ Runï¼Œé‡Œç¨‹ç¢‘è¦è¨»å†Šåœ¨ Catalogã€‚ã€

### å„ Job çš„ MLflow å‹•ä½œ

| éšæ®µ | MLflow å‹•ä½œ | Registry Alias | è¿½æº¯æ„ç¾© |
|------|------------|----------------|---------|
| **Job 2 (Pre-train)** | Log Model + Register | `Candidate-Pretrain` | ç´€éŒ„æ©Ÿå™¨äººå­¸æœƒã€Œèµ°è·¯ã€çš„éç¨‹ |
| **Job 3 (Conversion)** | Log Artifact | N/A | ç´€éŒ„ JAXâ†’PyTorch è½‰æ›çš„èª¤å·®æ•¸æ“š |
| **Job 4 (Fine-tune)** | Log Model + Register | `Candidate-Finetuned` | ç´€éŒ„å®˜æ–¹ç’°å¢ƒé©æ‡‰å¾Œçš„è¡¨ç¾ |
| **Gate 3 (Verification)** | æ›´æ–° Alias | `Champion` / `Finalist` | æ±ºè³½å€™é¸æ¨¡å‹ |

### ç‚ºä»€éº¼ã€Œæ¯å€‹ Job éƒ½è¨˜éŒ„ã€æ˜¯å¿…è¦çš„ï¼Ÿ

1. **å•é¡Œè¿½æº¯ï¼š** ç™¼ç¾å¾®èª¿è¡Œç‚ºå´©æ½°æ™‚ï¼Œèƒ½ä¸€éµæ‰¾å›æºé ­
2. **é¿å…ç¡¬ç·¨ç¢¼è·¯å¾‘éŒ¯èª¤ï¼š** ç”¨ Alias è¼‰å…¥æ¨¡å‹ï¼Œè€Œéæ‰‹å‹•è¼¸å…¥ Volume è·¯å¾‘
3. **è¡€ç·£è¿½è¹¤ï¼š** çŸ¥é“æ¨¡å‹æ˜¯ç”±å“ªå€‹ Notebook ç‰ˆæœ¬ç”¢ç”Ÿ

### Unity Catalog æ¨¡å‹é—œä¿‚

```
sac_actor (v1)  @Candidate-Pretrain
         â†“
    [jax2torch]  (logged as artifact)
         â†“
ddpg_pretrained (v1)  @Candidate-Finetuned
         â†“
    [fine-tune + Gate 3]
         â†“
ddpg_finetuned (v1)  @Champion â†’ SAI Submission
```

### è·¨ Job æ¨¡å‹å…±äº«ï¼ˆé¿å…ç¡¬ç·¨ç¢¼è·¯å¾‘ï¼‰

```python
# Job 3 è¼‰å…¥ Job 2 ç”¢å‡ºçš„æ¨¡å‹ï¼ˆä½¿ç”¨ Alias è€Œé Volume è·¯å¾‘ï¼‰
model = mlflow.models.load_model(
    "models:/booster_soccer.rl_models.sac_actor@Candidate-Pretrain"
)

# æ¯”è¼ƒï¼šé¿å…é€™ç¨®ç¡¬ç·¨ç¢¼æ–¹å¼
# model = load("/Volumes/.../final_v2_new_fixed.pkl")  # å®¹æ˜“å‡ºéŒ¯
```

### DualLogger å»ºè­°å¯¦ç¾

```python
class DualLogger:
    def __init__(self, wandb_project, mlflow_experiment):
        wandb.init(project=wandb_project)
        mlflow.set_experiment(mlflow_experiment)
        self.mlflow_run = mlflow.start_run()

    def log_model(self, model, model_name, register=False):
        """
        æ°¸é  Log åˆ° Run è£¡ï¼ˆç¢ºä¿æœ‰æ­·å²å¯ä»¥è¿½æº¯ï¼‰
        åªæœ‰åœ¨ register=True æ™‚æ‰è¨»å†Šåˆ° Unity Catalog
        """
        mlflow.pytorch.log_model(model, artifact_path="model")

        if register:
            mlflow.register_model(
                model_uri=f"runs:/{mlflow.active_run().info.run_id}/model",
                name=f"booster_soccer.rl_models.{model_name}"
            )

    def set_alias(self, model_name, version, alias):
        """ç‚ºé€šéé©—è­‰çš„æ¨¡å‹è¨­ç½® Alias"""
        from mlflow.tracking import MlflowClient
        client = MlflowClient()
        client.set_registered_model_alias(
            name=f"booster_soccer.rl_models.{model_name}",
            alias=alias,
            version=version
        )
```

### MLflow è¨˜éŒ„ Lineageï¼ˆJob 4 ç¯„ä¾‹ï¼‰

```python
# åœ¨ Job 4 (Fine-tuning) ä¸­
with mlflow.start_run(run_name="ddpg_finetune_v1") as run:
    # è¨˜éŒ„çˆ¶æ¨¡å‹è³‡è¨Š
    mlflow.set_tag("parent_model", "booster_soccer.rl_models.ddpg_pretrained")
    mlflow.set_tag("parent_alias", "Candidate-Pretrain")
    mlflow.set_tag("training_type", "finetuning")

    # è¨“ç·´å®Œæˆå¾Œ
    logger.log_model(model, "ddpg_finetuned", register=True)

    # å¦‚æœé€šé Gate 3 é©—è­‰
    if gate3_passed:
        logger.set_alias("ddpg_finetuned", run.info.run_id, "Champion")
```

### å„²å­˜ç©ºé–“ç®¡ç†

| å»ºè­° | èªªæ˜ |
|------|------|
| **æ¯å€‹ Job åªåœ¨çµæŸæ™‚è¨»å†Šä¸€æ¬¡** | ä¸­é–“éç¨‹ç”¨ `mlflow.log_artifact()` å­˜æˆæ™®é€šæ–‡ä»¶ |
| **å®šæœŸæ¸…ç†å¤±æ•— Run** | è¨­ç½®è…³æœ¬æˆ–åˆ©ç”¨ MLflow çš„ `deleted` æ¨™ç±¤ç®¡ç† |
| **Checkpoint å­˜ Volume** | é »ç¹å­˜æª”ç”¨ Volumeï¼Œåªæœ‰é‡Œç¨‹ç¢‘æ‰è¨»å†Šåˆ° Registry |

---

## å¿«é€Ÿé–‹å§‹

### 1. è¨­ç½® Unity Catalog

```sql
CREATE CATALOG IF NOT EXISTS booster_soccer;
CREATE SCHEMA IF NOT EXISTS booster_soccer.rl_models;
CREATE VOLUME IF NOT EXISTS booster_soccer.rl_models.checkpoints;
```

### 2. å»ºç«‹ Instance Pool (Train-Pool)

```bash
# ä½¿ç”¨ Databricks CLI å»ºç«‹ Instance Pool
databricks instance-pools create --json '{
  "instance_pool_name": "booster-train-pool",
  "node_type_id": "g2-standard-12",
  "min_idle_instances": 0,
  "max_capacity": 2,
  "idle_instance_autotermination_minutes": 60,
  "preloaded_spark_versions": ["16.4-gpu-ml-scala2.12"],
  "gcp_attributes": {
    "availability": "PREEMPTIBLE_GCP"
  }
}'

# è¨˜ä¸‹è¿”å›çš„ instance_pool_idï¼Œç”¨æ–¼ Job é…ç½®
```

### 3. æº–å‚™ Docker Image

åƒè€ƒ [01-environment-setup.md](./01-environment-setup.md) ä¸­çš„ Dockerfileï¼ŒBuild ä¸¦ Push åˆ°ä½ çš„ Registryï¼š

```bash
# Build
docker build -t your-registry/booster-rl:v1 .

# Push
docker push your-registry/booster-rl:v1
```

### 4. ä¸Šå‚³ Init Scriptï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰

å¦‚æœ Docker Image å°šæœªæº–å‚™å¥½ï¼Œå¯æš«æ™‚ä½¿ç”¨ Init Scriptï¼š

```bash
# å…ˆå»ºç«‹ scripts ç›®éŒ„ï¼ˆåœ¨ Unity Catalog Volume ä¸­ï¼‰
databricks fs mkdir /Volumes/booster_soccer/rl_models/scripts

# ä¸Šå‚³ Init Script åˆ° Unity Catalog Volumeï¼ˆé¿å… dbfs:/ åœ¨ UC Cluster è¢«ç¦ç”¨ï¼‰
databricks fs cp scripts/install_mjx.sh /Volumes/booster_soccer/rl_models/scripts/install_mjx.sh
```

### 5. å»ºç«‹ Workflow

```bash
databricks jobs create --json-file config/job_definitions.json
```

### 6. åŸ·è¡Œ Pipeline

```bash
databricks jobs run-now --job-id <JOB_ID>
```

### 7. ç›£æ§è¨“ç·´

- **W&B Dashboard:** å¯¦æ™‚æ›²ç·šå’Œå½±ç‰‡
- **MLflow UI:** æ¨¡å‹ç‰ˆæœ¬å’Œæ¯”è¼ƒ
- **Databricks Job UI:** Pipeline ç‹€æ…‹

---

## å¸¸è¦‹å•é¡Œ

### Q: GPU Job å•Ÿå‹•å¾ˆæ…¢ï¼Ÿ

**åŸå› ï¼š** Init script æ¯æ¬¡éƒ½é‡æ–°å®‰è£å¥—ä»¶

**è§£æ±ºï¼š** ä½¿ç”¨é è£å¥½å¥—ä»¶çš„ Docker Container æˆ– Cluster Policy

### Q: W&B ç„¡æ³•ä¸Šå‚³ï¼Ÿ

**åŸå› ï¼š** Databricks ç¶²è·¯é™åˆ¶

**è§£æ±ºï¼š** ä½¿ç”¨ offline modeï¼Œè¨“ç·´å¾ŒåŒæ­¥
```python
wandb.init(mode="offline")
# è¨“ç·´çµæŸå¾Œ
# wandb sync ./wandb/offline-run-*
```

### Q: Unity Catalog æ¬Šé™ä¸è¶³ï¼Ÿ

**åŸå› ï¼š** ç¼ºå°‘å¿…è¦æ¬Šé™

**è§£æ±ºï¼š** ç¢ºèªæœ‰ `CREATE MODEL`, `USE SCHEMA`, `USE CATALOG` æ¬Šé™

---

## è³‡æºé€£çµ

- [Databricks Jobs Documentation](https://docs.databricks.com/workflows/jobs/jobs.html)
- [Unity Catalog Model Registry](https://docs.databricks.com/aws/en/machine-learning/manage-model-lifecycle/)
- [MLflow on Databricks](https://docs.databricks.com/mlflow/index.html)
- [GPU Cluster Configuration](https://docs.databricks.com/compute/gpu.html)

---

## ä¸‹ä¸€æ­¥

1. å®Œæˆ [01-environment-setup.md](./01-environment-setup.md) çš„ç’°å¢ƒè¨­ç½®
2. å»ºç«‹ `databricks/workflows/` ç›®éŒ„çµæ§‹
3. ä¾åºå¯¦ä½œ 5 å€‹ Job Notebooks
4. æ¸¬è©¦ç«¯åˆ°ç«¯ Pipeline
5. è¿­ä»£å„ªåŒ–ä¸¦æäº¤ç«¶è³½
