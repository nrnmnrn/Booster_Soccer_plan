# å·¥å…·æ•´åˆ

## æ¦‚è¿°

æœ¬æ–‡ä»¶èªªæ˜å¦‚ä½•æ•´åˆ W&Bã€MLflowã€GPU ç›£æ§å’Œ Optuna é€²è¡Œå®Œæ•´çš„ MLOps å·¥ä½œæµç¨‹ã€‚

**æ ¸å¿ƒç­–ç•¥ï¼šW&B è² è²¬å¯¦é©—è¿½è¹¤ï¼ŒMLflow è² è²¬æ¨¡å‹ç®¡ç†**

---

## W&B + MLflow é›™è»Œè¿½è¹¤ç­–ç•¥

### æ ¸å¿ƒåŸå‰‡

> ã€Œæ­·å²è¦ç´€éŒ„åœ¨ Runï¼Œé‡Œç¨‹ç¢‘è¦è¨»å†Šåœ¨ Catalogã€‚ã€

- **W&B**ï¼šè¨“ç·´éç¨‹ä¸­çš„ã€Œçœ¼ç›ã€- çœ‹æ›²ç·šã€çœ‹å½±ç‰‡ã€å¿«é€Ÿè¿­ä»£
- **MLflow Run**ï¼šæ¯å€‹ Job éƒ½è¨˜éŒ„ï¼Œç¢ºä¿å®Œæ•´è¡€ç·£è¿½è¹¤
- **MLflow Registry**ï¼šåªåœ¨é—œéµç¯€é»è¨»å†Šï¼Œä¿æŒ Catalog æ•´æ½”

### åŠŸèƒ½åˆ†å·¥è¡¨

| åŠŸèƒ½ | W&B | MLflow |
|------|-----|--------|
| **å¯¦æ™‚è¨“ç·´æ›²ç·š** | âœ… ä¸»è¦ | å‚™æ´ |
| **GPU ç›£æ§** | âœ… wandb.log() | log_metric() |
| **å½±ç‰‡è¨˜éŒ„** | âœ… wandb.Video() | - |
| **è¶…åƒæ•¸è¨˜éŒ„** | âœ… config | log_params() |
| **æ¯å€‹ Job çš„ Run** | - | âœ… å¿…è¦ï¼ˆè¡€ç·£è¿½è¹¤ï¼‰ |
| **æ¨¡å‹ Registry** | artifact | âœ… åªè¨»å†Šé‡Œç¨‹ç¢‘ |
| **è·¨ Job æ¨¡å‹å…±äº«** | - | âœ… load_model(alias) |
| **Lineage è¿½è¹¤** | - | âœ… Unity Catalog |

### å„ Job çš„è¨˜éŒ„ç­–ç•¥

| éšæ®µ | W&B | MLflow Run | MLflow Registry |
|------|-----|-----------|-----------------|
| **Job 2 (Pre-train)** | å¯¦æ™‚æ›²ç·š + å½±ç‰‡ | âœ… Log Model | Register @Candidate-Pretrain |
| **Job 3 (Conversion)** | - | âœ… Log Artifact | N/A |
| **Job 4 (Fine-tune)** | å¯¦æ™‚æ›²ç·š | âœ… Log Model | Register @Candidate-Finetuned |
| **Gate 3 é€šé** | - | æ›´æ–° Tag | è¨­ç½® @Champion Alias |

è©³ç´°çš„ MLflow é›™è»Œè¿½è¹¤ç­–ç•¥è«‹è¦‹ [07-databricks-mlops.md](./07-databricks-mlops.md#æ¨¡å‹-lineage-è¿½è¹¤)ã€‚

---

## Weights & Biases (W&B) - å¯¦é©—è¿½è¹¤

### ç‚ºä»€éº¼ä½¿ç”¨ W&Bï¼Ÿ

- **RL è¨“ç·´æ¨™é…ï¼š** è¿½è¹¤ä¸ç©©å®šçš„è¨“ç·´éç¨‹
- **å½±ç‰‡è¨˜éŒ„ï¼š** ç›´æ¥åœ¨ç¶²é ä¸Šè§€çœ‹æ©Ÿå™¨äººè¡Œç‚º
- **åŸç”Ÿæ”¯æ´ï¼š** `imitation_learning/train.py` å·²æœ‰ W&B æ•´åˆ

### è¨­ç½®

```python
import wandb

# åˆå§‹åŒ–
wandb.init(
    project="booster_soccer",
    config={
        "algorithm": "SAC",
        "batch_size": 2048,
        "learning_rate": 3e-4,
        "total_timesteps": 10_000_000
    }
)
```

### è¨˜éŒ„æŒ‡æ¨™

```python
# åœ¨è¨“ç·´å¾ªç’°ä¸­
for step in range(total_timesteps):
    # ... è¨“ç·´é‚è¼¯ ...

    if step % 1000 == 0:
        wandb.log({
            "reward": episode_reward,
            "critic_loss": critic_loss,
            "actor_loss": actor_loss,
            "entropy": entropy,
            "step": step
        })
```

### å½±ç‰‡è¨˜éŒ„

```python
import time
from imitation_learning.utils.logging import get_wandb_video

class VideoLogger:
    """æŒ‰æ™‚é–“é–“éš”è¨˜éŒ„å½±ç‰‡ï¼Œé¿å… MJX é«˜ååé‡ä¸‹è¨˜éŒ„éæ–¼é »ç¹"""
    def __init__(self, interval_seconds=300):  # æ¯ 5 åˆ†é˜
        self.interval = interval_seconds
        self.last_log_time = 0

    def should_log(self):
        current_time = time.time()
        if current_time - self.last_log_time >= self.interval:
            self.last_log_time = current_time
            return True
        return False

# åˆå§‹åŒ–
video_logger = VideoLogger(interval_seconds=300)

# åœ¨è¨“ç·´å¾ªç’°ä¸­ä½¿ç”¨
if video_logger.should_log():
    # æ”¶é›† render frames
    renders = collect_episode_renders(env, model)

    # ä¸Šå‚³åˆ° W&B
    wandb.log({
        "video": get_wandb_video(renders, fps=30),
        "step": step
    })
```

> **è¨­è¨ˆèªªæ˜ï¼š** ä½¿ç”¨æ™‚é–“é–“éš”ï¼ˆè€Œéå›ºå®šæ­¥æ•¸ï¼‰ä¾†è¨˜éŒ„å½±ç‰‡ï¼Œå› ç‚º MJX (2048 ä¸¦è¡Œç’°å¢ƒ) çš„ååé‡å¯èƒ½é”åˆ° 1M+ steps/minã€‚å›ºå®š 50,000 æ­¥å¯èƒ½æ¯åˆ†é˜è¨˜éŒ„å¤šæ¬¡å½±ç‰‡ï¼Œå°è‡´ W&B å„²å­˜ç©ºé–“çˆ†ç‚¸ã€‚æ¯ 5 åˆ†é˜è¨˜éŒ„ä¸€æ¬¡æ›´åŠ ç©©å®šã€‚

### æ•´åˆåˆ° RL è¨“ç·´è…³æœ¬

```python
# training_scripts/training.py ä¿®æ”¹

import wandb

def training_loop(env, model, action_function, preprocess_class, timesteps=1000):
    # åˆå§‹åŒ– W&B
    wandb.init(project="booster_soccer_rl")

    replay_buffer = ReplayBuffer(max_size=100000)
    preprocessor = preprocess_class()

    for total_steps in range(timesteps):
        # ... è¨“ç·´é‚è¼¯ ...

        # è¨˜éŒ„åˆ° W&B
        if total_steps % 1000 == 0:
            wandb.log({
                "episode_reward": episode_reward,
                "critic_loss": critic_loss,
                "actor_loss": actor_loss,
                "buffer_size": len(replay_buffer)
            })

    wandb.finish()
```

---

## MLflow - æ¨¡å‹ç®¡ç†ï¼ˆUnity Catalogï¼‰

### åŸºæœ¬è¨­ç½®

```python
import mlflow

# è¨­ç½® Unity Catalog ä½œç‚º Model Registry
mlflow.set_registry_uri("databricks-uc")

# è¨­ç½®å¯¦é©—
mlflow.set_experiment("/Users/<username>/booster_soccer_mjx")
```

### èˆ‡ W&B ä¸¦è¡Œä½¿ç”¨

```python
import wandb
import mlflow

# åŒæ™‚è¨˜éŒ„åˆ°å…©å€‹ç³»çµ±
with mlflow.start_run(run_name="mjx_sac_v1") as mlflow_run:
    wandb.init(project="booster_soccer", name="mjx_sac_v1")

    mlflow.log_params(config)  # MLflow
    wandb.config.update(config)  # W&B

    for step in range(total_steps):
        metrics = {"reward": reward, "loss": loss}

        wandb.log(metrics, step=step)  # W&B - å¯¦æ™‚
        for k, v in metrics.items():
            mlflow.log_metric(k, v, step=step)  # MLflow - å‚™æ´

    # æ¨¡å‹è¨»å†Šåˆ° Unity Catalog
    mlflow.pytorch.log_model(
        model,
        artifact_path="model",
        registered_model_name="booster_soccer.rl_models.mjx_sac"
    )

    wandb.finish()
```

### æ¨¡å‹ç‰ˆæœ¬æ¯”è¼ƒ

```python
from mlflow.tracking import MlflowClient

client = MlflowClient()
model_name = "booster_soccer.rl_models.ddpg_finetuned"

# åˆ—å‡ºæ‰€æœ‰ç‰ˆæœ¬
versions = client.search_model_versions(f"name='{model_name}'")
for v in versions:
    print(f"Version {v.version}: {v.current_stage}")
```

---

## GPU ç›£æ§

### ç‚ºä»€éº¼éœ€è¦ GPU ç›£æ§ï¼Ÿ

- **L4 GPU æˆæœ¬æ§åˆ¶ï¼š** å³æ™‚ç™¼ç¾æ•ˆèƒ½ç“¶é ¸
- **OOM é è­¦ï¼š** åœ¨å´©æ½°å‰ç™¼ç¾è¨˜æ†¶é«”å•é¡Œ
- **è¨“ç·´æ•ˆç‡åˆ†æï¼š** ç¢ºèª GPU åˆ©ç”¨ç‡é”åˆ°é æœŸ

### GPU ç›£æ§å¿…è¦æŒ‡æ¨™ï¼ˆå¿…é ˆåŒ…å«ï¼‰

è¨“ç·´è…³æœ¬ **å¿…é ˆ** åŒ…å«ä»¥ä¸‹ç›£æ§æŒ‡æ¨™ï¼š

| æŒ‡æ¨™ | è®Šæ•¸å | èªªæ˜ |
|------|--------|------|
| GPU åˆ©ç”¨ç‡ | `gpu/utilization_percent` | ç¢ºèª GPU è¢«å……åˆ†ä½¿ç”¨ |
| VRAM ä½¿ç”¨é‡ | `gpu/memory_percent` | é è­¦ OOM é¢¨éšª |

```python
# æ¯ 1000 æ­¥å¿…é ˆè¨˜éŒ„
wandb.log({
    "gpu/utilization_percent": gpu_util,
    "gpu/memory_percent": vram_usage,
    "step": step
})
```

> å¦‚æœ GPU åˆ©ç”¨ç‡é•·æœŸä½æ–¼ 80%ï¼Œæ‡‰æª¢æŸ¥æ˜¯å¦æœ‰ I/O ç“¶é ¸æˆ– batch size éå°ã€‚

### JAX/XLA è¨˜æ†¶é«”ç®¡ç†

JAX é è¨­æœƒä½”ç”¨ **æ‰€æœ‰å¯ç”¨ GPU è¨˜æ†¶é«”**ï¼Œé€™æœƒå°è‡´ï¼š
- PyTorch è½‰æ›éšæ®µ OOM
- MuJoCo EGL æ¸²æŸ“å¤±æ•—
- ç„¡æ³•åœ¨åŒä¸€ GPU ä¸Šé‹è¡Œå…¶ä»–ç¨‹åº

**å¿…é ˆè¨­ç½®çš„ç’°å¢ƒè®Šæ•¸ï¼š**

```python
import os
# åœ¨ import jax ä¹‹å‰è¨­ç½®ï¼
os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.75'  # åªä½¿ç”¨ 75% VRAM
os.environ['JAX_PREALLOCATE'] = 'false'                 # å‹•æ…‹åˆ†é…ï¼Œé¿å…ç¢ç‰‡åŒ–
```

| ç’°å¢ƒè®Šæ•¸ | å»ºè­°å€¼ | èªªæ˜ |
|---------|-------|------|
| `XLA_PYTHON_CLIENT_MEM_FRACTION` | `0.75` | L4 24GB Ã— 0.75 = 18GB çµ¦ JAX |
| `JAX_PREALLOCATE` | `false` | å‹•æ…‹åˆ†é…ï¼Œé¿å…å•Ÿå‹•æ™‚ä½”æ»¿ VRAM |
| `XLA_FLAGS` | `--xla_gpu_cuda_data_dir=/usr/local/cuda` | XLA ç·¨è­¯ç·©å­˜è·¯å¾‘ |

> ğŸ’¡ åœ¨ Init Script ä¸­å·²é è¨­é€™äº›å€¼ã€‚è©³è¦‹ [07-databricks-mlops.md](./07-databricks-mlops.md)ã€‚

### ç›£æ§è¨­è¨ˆ

```python
import pynvml

class GPUMonitor:
    def __init__(self, alert_memory_threshold=0.95, alert_temp=85):
        pynvml.nvmlInit()
        self.handle = pynvml.nvmlDeviceGetHandleByIndex(0)
        self.threshold = alert_memory_threshold
        self.alert_temp = alert_temp

    def get_metrics(self):
        mem = pynvml.nvmlDeviceGetMemoryInfo(self.handle)
        util = pynvml.nvmlDeviceGetUtilizationRates(self.handle)
        temp = pynvml.nvmlDeviceGetTemperature(
            self.handle, pynvml.NVML_TEMPERATURE_GPU
        )

        return {
            "gpu/memory_used_gb": mem.used / 1e9,
            "gpu/memory_percent": 100 * mem.used / mem.total,
            "gpu/utilization_percent": util.gpu,
            "gpu/temperature": temp,
        }

    def log_and_alert(self, step):
        metrics = self.get_metrics()

        # è¨˜éŒ„åˆ° W&B
        wandb.log(metrics, step=step)

        # è­¦å ±æª¢æŸ¥
        if metrics["gpu/memory_percent"] > self.threshold * 100:
            wandb.alert(
                title="High GPU Memory",
                text=f"{metrics['gpu/memory_percent']:.1f}%"
            )

        return metrics
```

### ç›£æ§é »ç‡å»ºè­°

| é–“éš” | ç›£æ§å…§å®¹ |
|------|----------|
| æ¯ 1000 æ­¥ | åŸºæœ¬ metrics (memory, utilization) |
| æ¯ 5000 æ­¥ | å®Œæ•´ç‹€æ…‹ (+ temperature, power) |
| æ¯ 200k æ­¥ | Checkpoint + ç³»çµ±å¿«ç…§ï¼ˆé…åˆ Preemptibleï¼‰ |

### æ•´åˆåˆ°è¨“ç·´å¾ªç’°

```python
monitor = GPUMonitor()

for step in range(total_steps):
    # è¨“ç·´é‚è¼¯...

    # GPU ç›£æ§
    if step % 1000 == 0:
        gpu_metrics = monitor.log_and_alert(step)

    # Checkpointï¼ˆæ¯ 200k æ­¥ï¼Œé…åˆ Preemptible ç¸®çŸ­é–“éš”ï¼‰
    if step % 200000 == 0:
        save_checkpoint(model, step)
        mlflow.log_artifact(checkpoint_path)
```

---

## Optuna è¶…åƒæ•¸èª¿å„ª

### ç‚ºä»€éº¼ä½¿ç”¨ Optunaï¼Ÿ

- **è¼•é‡ç´šï¼š** æ¯” Ray Tune æ›´é©åˆå–® GPU
- **å‰ªæåŠŸèƒ½ï¼š** è‡ªå‹•åœæ­¢å·®çš„å¯¦é©—ï¼Œç¯€çœ GPU è²»ç”¨
- **RL æ”¯æ´ï¼š** CleanRL æœ‰å®˜æ–¹æ•´åˆç¯„ä¾‹

### è¨­ç½®

```python
import optuna

def objective(trial):
    # å®šç¾©è¶…åƒæ•¸æœç´¢ç©ºé–“
    lr = trial.suggest_float("lr", 1e-5, 1e-3, log=True)
    batch_size = trial.suggest_categorical("batch_size", [128, 256, 512, 1024])
    tau = trial.suggest_float("tau", 0.001, 0.01)
    gamma = trial.suggest_float("gamma", 0.95, 0.999)

    # å»ºç«‹æ¨¡å‹å’Œç’°å¢ƒ
    model = DDPG_FF(
        n_features=87,
        action_space=env.action_space,
        neurons=[256, 256],
        learning_rate=lr
    )

    # è¨“ç·´
    total_reward = 0
    for step in range(100000):
        # ... è¨“ç·´é‚è¼¯ ...

        # å®šæœŸå ±å‘Šä¸­é–“çµæœï¼ˆç”¨æ–¼å‰ªæï¼‰
        if step % 10000 == 0:
            trial.report(total_reward / (step + 1), step)
            if trial.should_prune():
                raise optuna.TrialPruned()

    return total_reward

# å»ºç«‹ study ä¸¦å„ªåŒ–
study = optuna.create_study(
    direction="maximize",
    pruner=optuna.pruners.MedianPruner(n_startup_trials=5)
)
study.optimize(objective, n_trials=50)

# ç²å–æœ€ä½³è¶…åƒæ•¸
print("Best params:", study.best_params)
print("Best value:", study.best_value)
```

### èˆ‡ W&B æ•´åˆ

```python
import optuna
import wandb

def objective(trial):
    # åˆå§‹åŒ– W&B run
    wandb.init(
        project="booster_soccer_tuning",
        config={
            "lr": trial.suggest_float("lr", 1e-5, 1e-3, log=True),
            "batch_size": trial.suggest_categorical("batch_size", [128, 256, 512]),
        },
        reinit=True
    )

    # è¨“ç·´
    for step in range(100000):
        # ... è¨“ç·´é‚è¼¯ ...
        wandb.log({"reward": reward, "step": step})

    wandb.finish()
    return final_reward
```

### Optuna Dashboard

```bash
# å®‰è£
pip install optuna-dashboard

# å•Ÿå‹• dashboard
optuna-dashboard sqlite:///optuna_study.db
```

---

## å·¥å…·å„ªå…ˆç´š

| å·¥å…· | å„ªå…ˆç´š | ä½•æ™‚ä½¿ç”¨ |
|------|--------|----------|
| **W&B** | P0 (å¿…è¦) | å¾ Day 1 é–‹å§‹ |
| **Optuna** | P1 (æ¨è–¦) | åŸºç¤è¨“ç·´è·‘é€šå¾Œ |
| **TensorBoard** | P2 (å¯é¸) | é›¢ç·šç’°å¢ƒå‚™ç”¨ |

---

## å¸¸è¦‹å•é¡Œ

### Q: W&B åœ¨ Databricks ç„¡æ³•ä¸Šå‚³ï¼Ÿ

**å¸¸è¦‹åŸå› ï¼š** Databricks Worker ç¯€é»é˜²ç«ç‰†é˜»æ“‹ W&B ä¸Šå‚³ç«¯å£ã€‚

**ç—‡ç‹€ï¼š**
- è¨“ç·´é€Ÿåº¦ç•°å¸¸æ…¢ï¼ˆæ¯ç§’æ­¥æ•¸é©Ÿé™ï¼‰
- `wandb.log()` åŸ·è¡Œæ™‚é–“è¶…éæ•¸ç§’
- è¨“ç·´å¡ä½ä¸å‹•

**è§£æ±ºæ–¹æ¡ˆ 1ï¼šæ‰‹å‹• Offline æ¨¡å¼**

```python
# è¨­ç½® offline æ¨¡å¼
wandb.init(mode="offline")

# è¨“ç·´çµæŸå¾ŒåŒæ­¥
wandb.finish()
# ç„¶å¾Œæ‰‹å‹•ä¸Šå‚³: wandb sync ./wandb/offline-run-*
```

**è§£æ±ºæ–¹æ¡ˆ 2ï¼šè‡ªå‹•é€£ç·šåµæ¸¬ï¼ˆæ¨è–¦ï¼‰**

```python
import wandb
import socket
import time

def init_wandb_with_fallback(project, config, timeout=10):
    """
    åˆå§‹åŒ– W&Bï¼Œè‡ªå‹•åµæ¸¬é€£ç·šå•é¡Œä¸¦åˆ‡æ› offline æ¨¡å¼

    Args:
        project: W&B å°ˆæ¡ˆåç¨±
        config: è¨“ç·´é…ç½®
        timeout: é€£ç·šè¶…æ™‚ç§’æ•¸

    Returns:
        wandb run object
    """
    def check_wandb_connection():
        try:
            socket.create_connection(("api.wandb.ai", 443), timeout=timeout)
            return True
        except (socket.timeout, OSError):
            return False

    if check_wandb_connection():
        try:
            run = wandb.init(project=project, config=config)
            print("âœ… W&B é€£ç·šæˆåŠŸ")
            return run
        except Exception as e:
            print(f"âš ï¸ W&B åˆå§‹åŒ–å¤±æ•—: {e}ï¼Œåˆ‡æ› offline æ¨¡å¼")
    else:
        print("âš ï¸ ç„¡æ³•é€£ç·š W&Bï¼Œä½¿ç”¨ offline æ¨¡å¼")

    # Fallback to offline
    run = wandb.init(project=project, config=config, mode="offline")
    print("ğŸ“¦ W&B offline æ¨¡å¼å•Ÿç”¨")
    print("   è¨“ç·´å¾Œè«‹æ‰‹å‹•åŒæ­¥: wandb sync ./wandb/offline-run-*")
    return run
```

**è§£æ±ºæ–¹æ¡ˆ 3ï¼šå¸¶å»¶é²åµæ¸¬çš„ Logger**

```python
class WandbLogger:
    """
    å¸¶æœ‰è‡ªå‹• offline fallback å’Œå»¶é²åµæ¸¬çš„ W&B Logger
    """
    def __init__(self, project, config):
        self.run = init_wandb_with_fallback(project, config)
        self.is_offline = self.run.mode == "offline"
        self.log_warning_shown = False

    def log(self, metrics, step=None):
        """
        è¨˜éŒ„ metricsï¼Œåµæ¸¬ä¸Šå‚³å»¶é²
        """
        start = time.time()
        wandb.log(metrics, step=step)
        elapsed = time.time() - start

        # å¦‚æœå–®æ¬¡ log è¶…é 2 ç§’ï¼Œç™¼å‡ºè­¦å‘Š
        if elapsed > 2.0 and not self.log_warning_shown:
            print(f"âš ï¸ W&B log è€—æ™‚ {elapsed:.1f}sï¼Œå¯èƒ½æœ‰ç¶²è·¯ç“¶é ¸")
            print("   è€ƒæ…®åˆ‡æ› offline æ¨¡å¼æˆ–æ¸›å°‘ log é »ç‡")
            self.log_warning_shown = True

    def finish(self):
        """çµæŸè¨˜éŒ„"""
        wandb.finish()
        if self.is_offline:
            print("ğŸ“¦ è«‹åŸ·è¡Œ: wandb sync ./wandb/offline-run-*")
```

**ä½¿ç”¨ç¯„ä¾‹ï¼š**

```python
# å–ä»£åŸæœ¬çš„ wandb.init()
logger = WandbLogger(project="booster_soccer_mjx", config=config)

# è¨“ç·´å¾ªç’°ä¸­
for step in range(total_steps):
    # ... è¨“ç·´é‚è¼¯ ...
    if step % 1000 == 0:
        logger.log({"reward": reward, "loss": loss}, step=step)

# çµæŸ
logger.finish()
```

### Q: Optuna è©¦é©—å¤ªæ…¢ï¼Ÿ

```python
# ä½¿ç”¨æ›´æ¿€é€²çš„å‰ªæ
pruner = optuna.pruners.MedianPruner(
    n_startup_trials=3,      # æ¸›å°‘å•Ÿå‹•è©¦é©—æ•¸
    n_warmup_steps=5000,     # æ¸›å°‘é ç†±æ­¥æ•¸
    interval_steps=5000      # æ›´é »ç¹æª¢æŸ¥
)
```

### Q: å¦‚ä½•æ¢å¾©ä¸­æ–·çš„ Optuna studyï¼Ÿ

```python
# ä½¿ç”¨æŒä¹…åŒ– storage
study = optuna.create_study(
    study_name="booster_soccer",
    storage="sqlite:///optuna_study.db",
    load_if_exists=True  # æ¢å¾©å·²æœ‰ study
)
```

---

## è³‡æºé€£çµ

- [W&B Documentation](https://docs.wandb.ai/)
- [W&B Video Logging](https://docs.wandb.ai/ref/python/data-types/video/)
- [Optuna Documentation](https://optuna.readthedocs.io/)
- [CleanRL Hyperparameter Tuning](https://docs.cleanrl.dev/advanced/hyperparameter-tuning/)
- [Optuna Examples for RL](https://github.com/optuna/optuna-examples/blob/main/rl/sb3_simple.py)
